{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02bbe7d",
   "metadata": {},
   "source": [
    "## NLP Zero to Hero\n",
    "### Machine Learning with Recurrent Neural Networks(RNN基本原理:一个数值可在整个数列的生命中反复出现)\n",
    "**简单的RNN, 有点像Fibonacci sequence(斐波那契数列)(1 2 3 5 8 13 21...), 序列很强大, 但随着计算的传递, 早期输入的影响就会越小, 即关键描述词越接近预测文本, 预测的越准确**\n",
    "<br><br>\n",
    "**但还有一种情况如, I live in China, so at school I communicate with people in <...>, 根据线索China, 预测为chinese. 若只根据距离近的单词来预测,会漏掉这一线索. 此处的关键是超越RNN的very short memory(超短期记忆), 形成longer short memory, 这种适用于RNN的算法通常被称为Long Short Term Memory(LSTM)(长短期记忆), LSTM引入了一种叫做'cell state'的概念, 它可以跨越多个时间段保持语境, 从而将句子开头的句意带入下文. 更神奇的是, 它也可以是双向的, 句子后面的词也可为前面的词提供语境, 这样就可更准确学习句子语义了**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864b8f8",
   "metadata": {},
   "source": [
    "### Long Short Term Memory for NLP\n",
    "**下面将学习如何在长句子中, 建立对文本上下文的理解, 并从中了解到, 一个句子中的前文单词可以决定下文的含义和语义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de92a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#获取Tokenizer API\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5067ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6d8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          640000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 714,369\n",
      "Trainable params: 714,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#如何在文本分类器中使用LSTM, 结果可以看到LSTM可以用到很多参数\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    #LSTM中的参数表示隐藏节点的数量, 也表示该LSTM输出的维度\n",
    "    #Bidirectional()表示将同时向前和向后巡视句子文本, 学习每个句子的最佳参数然后将其合并, 它可能无法满足所有场景需求, 但很值得尝试\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), \n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "#to print a useful summary of the model\n",
    "model.summary()\n",
    "#Note: LSTM层有128个参数, 因为是双向LSTM, 所以每个方向使用64个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725ae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          640000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 128)        66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 751,489\n",
      "Trainable params: 751,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#如何在文本分类器中使用LSTM, 结果可以看到LSTM可以用到很多参数\n",
    "#也可将LSTM层堆叠起来\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    #LSTM中的参数表示隐藏节点的数量, 也表示该LSTM输出的维度\n",
    "    #Bidirectional()表示将同时向前和向后巡视句子文本, 学习每个句子的最佳参数然后将其合并, 它可能无法满足所有场景需求, 但很值得尝试\n",
    "    #也可将LSTM层堆叠起来, 上一层的输出就会被传递到下一层, 这和Dense layers很像\n",
    "    #确保对所有逐层传递的图层都设置return_sequences = True, 这样的话, 假设有两层, 那么第一层输出应该包含在其中, 若有三个LSTM层堆叠, 前两个层应该被包含其中, 以此类推\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True)), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "#to print a useful summary of the model\n",
    "model.summary() #结果显示额外的LSTM导致了额外的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d4af1",
   "metadata": {},
   "source": [
    "## Training an AI to create poetry\n",
    "### 创建一个模型, 并用传统爱尔兰歌曲的歌词对它进行训练, 借助这一过程, 将尝试用这些单词创建属于自己的诗歌\n",
    "***步骤 :***\n",
    "1. 创建文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5e6c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the town of athy one jeremy lanigan ', ' battered away til he hadnt a pound. ', 'his father traveled and made him a man again ', ' left him a farm and ten acres of ground. ', 'he gave a grand party for friends and relations ', ' who didnt forget him when come to the wall, ', 'and if youll but listen lll make your eyes glisten ', \" of the rows and the ructions of lanigan's ball. \", 'myself to be sure got free invitation, ', ' for all the nice girls and boys i might ask, ', 'and just in a minute both friends and relations ', ' were dancing round merry as bees round a cask. ', 'judy odaly, that nice little milliner, ', ' she tipped me a wink for to give her a call, ', 'and i soon arrived with peggy mcgilligan ', \" just in time for lanigan's ball.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(为何不直接在tokenizer = Tokenizer()中用 <OOV> token编码? \\n在生成文本时, 这里与先前分类时的场景相比, 有细微区别: 生成文本时该示例不需要验证集, 而是要利用所有字节, 试着找出单词在哪里出现及如何出现的模式.\\n因此若对整个corpus(语料库)进行分词, 当然也就不会存在<OOV> token了)\\n稍后会看到从完整corpus的哪里开始填充字句, 为此我们需要一个 zero token, 因此我们在这里 +1, 把这个token算作一个有效词'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "#为简单起见, 只用了一首(诗)歌: Lanigan's Ball\n",
    "#被存储为单一字符串 通过\\n来分行 \n",
    "#注:string中character(字符)(每行一对单/双引号或整体string有一对单/双引号,作用差不多)多需要用多行写时, 通过每行结尾加'\\'来练成一个整体string\n",
    "data = \"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\n\" \\\n",
    "       \"His father traveled and made him a man again \\n Left him a farm and ten acres of ground. \\n\" \\\n",
    "       \"He gave a grand party for friends and relations \\n Who didnt forget him when come to the wall, \\n\" \\\n",
    "       \"And if youll but listen lll make your eyes glisten \\n Of the rows and the ructions of Lanigan's Ball. \\n\" \\\n",
    "       \"Myself to be sure got free invitation, \\n For all the nice girls and boys I might ask, \\n\" \\\n",
    "       \"And just in a minute both friends and relations \\n Were dancing round merry as bees round a cask. \\n\" \\\n",
    "       \"Judy ODaly, that nice little milliner, \\n She tipped me a wink for to give her a call, \\n\" \\\n",
    "       \"And I soon arrived with Peggy McGilligan \\n Just in time for Lanigan's Ball.\"\n",
    "\n",
    "#第二种写法(将每行末尾'\\'换成整体外围'()')\n",
    "# data = (\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\n\" \n",
    "#        \"His father traveled and made him a man again \\n Left him a farm and ten acres of ground. \\n\" \n",
    "#        \"He gave a grand party for friends and relations \\n Who didnt forget him when come to the wall, \\n\" \n",
    "#        \"And if youll but listen lll make your eyes glisten \\n Of the rows and the ructions of Lanigan's Ball. \\n\" \n",
    "#        \"Myself to be sure got free invitation, \\n For all the nice girls and boys I might ask, \\n\" \n",
    "#        \"And just in a minute both friends and relations \\n Were dancing round merry as bees round a cask. \\n\" \n",
    "#        \"Judy ODaly, that nice little milliner, \\n She tipped me a wink for to give her a call, \\n\" \n",
    "#        \"And I soon arrived with Peggy McGilligan \\n Just in time for Lanigan's Ball.\")\n",
    "       \n",
    "#print(data)\n",
    "corpus = data.lower().split('\\n')\n",
    "print(corpus)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1 #由于会用到一个 <OOV> token, 因此在单词索引长度上 +1\n",
    "'''(为何不直接在tokenizer = Tokenizer()中用 <OOV> token编码? \n",
    "在生成文本时, 这里与先前分类时的场景相比, 有细微区别: 生成文本时该示例不需要验证集, 而是要利用所有字节, 试着找出单词在哪里出现及如何出现的模式.\n",
    "因此若对整个corpus(语料库)进行分词, 当然也就不会存在<OOV> token了)\n",
    "稍后会看到从完整corpus的哪里开始填充字句, 为此我们需要一个 zero token, 因此我们在这里 +1, 把这个token算作一个有效词'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a04b1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nE.g.\\nA sequence: [0 0 0 0 4 2 66 8 67 68 69 70]\\n         X: [0 0 0 0 4 2 66 8 67 68 69]\\n     Label: [70]\\n         Y: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n             ...\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\\n上述Y有15行, 18列, 发现Y list(列表)中第70个元素是1.(因为索引从0开始(或者One-Hot编码是这样), 只在索引为70的位置是1., 其余都为0.)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将经过分词处理的句子列表转换成训练数据\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    #为corpus中的每行文本创建token列表(注:不是对整个corpus进行文本转序列, 而是一次只转换一行文本)\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    #通过这个token_list生成n-grams, 之所以这样, 是因为要训练的model是用来预测可能出现的下一个单词的\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "'''\n",
    "E.g.\n",
    "In the town of Athy one Jeremy Lanigan\n",
    "Line/token_list为[4 2 66 8 67 68 69 70]\n",
    "生成的n_gram_sequence一个个append后的Input Sequences:\n",
    "[4 2]\n",
    "[4 2 66]\n",
    "[4 2 66 8]\n",
    "[4 2 66 8 67]\n",
    "[4 2 66 8 67 68]\n",
    "[4 2 66 8 67 68 69]\n",
    "[4 2 66 8 67 68 69 70]\n",
    "'''\n",
    "\n",
    "#现已把句子分成多个list, 下一步要对其进行填充\n",
    "#首先, 获取最长句子的长度\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "#然后对长度不够的句子, 填充0来补足\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "'''\n",
    "经上一步的填充,现在这一行的8个单词已经形成了类似的7个列表\n",
    "Line: [4 2 66 8 67 68 69 70]\n",
    "Padded Input Sequences:\n",
    "[0 0 0 0 0 0 0 0 0 0 4 2]\n",
    "[0 0 0 0 0 0 0 0 0 4 2 66]\n",
    "[0 0 0 0 0 0 0 0 4 2 66 8]\n",
    "[0 0 0 0 0 0 0 4 2 66 8 67]\n",
    "[0 0 0 0 0 0 4 2 66 8 67 68]\n",
    "[0 0 0 0 0 4 2 66 8 67 68 69]\n",
    "[0 0 0 0 4 2 66 8 67 68 69 70]\n",
    "这非常理想, 给我们提供了features(X) and labels(Y) : 可把上述每个Sequence中除了最后一个值以外的所有token都视为X, 并把最后一个值看作Y\n",
    "如, [0 0 0 0 0 0 0 4 2 66 8 67]中 [0 0 0 0 0 0 0 4 2 66 8] 看作 Input(X), [67] 看作 Label(Y)\n",
    "'''\n",
    "#对上述list进行slice(分割)以实现训练集(Input(X) and Label(Y))\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1]\n",
    "'''再而希望Y被categorical(分类的, 绝对的, 直截了当的)且是One-Hot encoding(独热编码), 这样一来当训练时, \n",
    "就能在corpus的所有单词中推理出在当前单词集情况下, 哪个单词最有可能成为序列中的下一个单词, 这可借助keras.utils.to_categorical来实现'''\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
    "\n",
    "'''\n",
    "E.g.\n",
    "A sequence: [0 0 0 0 4 2 66 8 67 68 69 70]\n",
    "         X: [0 0 0 0 4 2 66 8 67 68 69]\n",
    "     Label: [70]\n",
    "         Y: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "             ...\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "上述Y有15行, 18列, 发现Y list(列表)中第70个元素是1.(因为索引从0开始(或者One-Hot编码是这样), 只在索引为70的位置是1., 其余都为0.)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4561432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Py\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 52ms/step - loss: 4.6022 - accuracy: 0.0168\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 4.4000 - accuracy: 0.0756\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 4.0281 - accuracy: 0.1261\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 3.1901 - accuracy: 0.1765\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.5258 - accuracy: 0.2773\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.9015 - accuracy: 0.3697\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.4049 - accuracy: 0.5798\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.9354 - accuracy: 0.7059\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.5902 - accuracy: 0.8992\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4064 - accuracy: 0.8908\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.2692 - accuracy: 0.9244\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.2282 - accuracy: 0.9412\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.2345 - accuracy: 0.9328\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1293 - accuracy: 0.9664\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0902 - accuracy: 0.9664\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0683 - accuracy: 0.9832\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.0594 - accuracy: 0.9832\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0522 - accuracy: 0.9832\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0492 - accuracy: 0.9748\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0431 - accuracy: 0.9748\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0409 - accuracy: 0.9832\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0369 - accuracy: 0.9748\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0356 - accuracy: 0.9748\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0359 - accuracy: 0.9832\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0342 - accuracy: 0.9832\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0372 - accuracy: 0.9832\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0369 - accuracy: 0.9832\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0322 - accuracy: 0.9748\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0466 - accuracy: 0.9832\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0382 - accuracy: 0.9832\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0313 - accuracy: 0.9832\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.0352 - accuracy: 0.9832\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0371 - accuracy: 0.9748\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0306 - accuracy: 0.9832\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0347 - accuracy: 0.9832\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0311 - accuracy: 0.9832\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0379 - accuracy: 0.9832\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0388 - accuracy: 0.9832\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0405 - accuracy: 0.9748\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0374 - accuracy: 0.9748\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0343 - accuracy: 0.9832\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0306 - accuracy: 0.9832\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0335 - accuracy: 0.9832\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0327 - accuracy: 0.9832\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0332 - accuracy: 0.9748\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 0.0318 - accuracy: 0.9832\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0308 - accuracy: 0.9832\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0302 - accuracy: 0.9832\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0318 - accuracy: 0.9832\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.0307 - accuracy: 0.9748\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0317 - accuracy: 0.9832\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0316 - accuracy: 0.9832\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0313 - accuracy: 0.9748\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0346 - accuracy: 0.9748\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0354 - accuracy: 0.9748\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0311 - accuracy: 0.9832\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0347 - accuracy: 0.9832\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0314 - accuracy: 0.9748\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0317 - accuracy: 0.9832\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0307 - accuracy: 0.9832\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0319 - accuracy: 0.9832\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0302 - accuracy: 0.9832\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0308 - accuracy: 0.9748\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0315 - accuracy: 0.9832\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0315 - accuracy: 0.9832\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0319 - accuracy: 0.9748\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0309 - accuracy: 0.9832\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0296 - accuracy: 0.9832\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0321 - accuracy: 0.9832\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0332 - accuracy: 0.9748\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0293 - accuracy: 0.9832\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0293 - accuracy: 0.9832\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0348 - accuracy: 0.9832\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0302 - accuracy: 0.9832\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0292 - accuracy: 0.9832\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0302 - accuracy: 0.9832\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.0307 - accuracy: 0.9832\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0311 - accuracy: 0.9832\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0304 - accuracy: 0.9748\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0308 - accuracy: 0.9748\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.0336 - accuracy: 0.9748\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0301 - accuracy: 0.9832\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0318 - accuracy: 0.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0306 - accuracy: 0.9832\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0294 - accuracy: 0.9832\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0308 - accuracy: 0.9748\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0303 - accuracy: 0.9832\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.0293 - accuracy: 0.9832\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0319 - accuracy: 0.9748\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0299 - accuracy: 0.9832\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0310 - accuracy: 0.9748\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0293 - accuracy: 0.9748\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0312 - accuracy: 0.9832\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0288 - accuracy: 0.9832\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0293 - accuracy: 0.9748\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0299 - accuracy: 0.9748\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0285 - accuracy: 0.9832\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0317 - accuracy: 0.9748\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 0.0288 - accuracy: 0.9832\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0298 - accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, Dense\n",
    "\n",
    "#用上述所有数据训练一个NN, 这里用一个非常简单的model architecture(模型架构)来实现, 它还未经过任何优化尤其中间层, 之后可自由进行尝试和改进\n",
    "model = Sequential()\n",
    "#Embedding中, 由于单词众多因此赋予它多个维度(此例中设置为240); \n",
    "#第一个参数代表corpus中的单词数; input_length = max_sequence_len-1 因为每个序列中的最后一个值都去掉作了label作为训练数据\n",
    "model.add(Embedding(total_words, 240, input_length = max_sequence_len-1))\n",
    "model.add(Bidirectional(tf.keras.layers.LSTM(150)))\n",
    "#很重要的一点是, output is a dense with total number of words, 另外, 由于label是One-Hot encoded,so we want an output that is represented of this\n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.01)\n",
    "#由于这被分成了很多类,将需要一个分类损失函数,比如categorical_crossentropy\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "#train时,可能会看到一开始的accuracy非常低, 别担心, accuracy会随时间的推移而提高, 结果表示最终accuracy大概能达到70%-75%左右\n",
    "history = model.fit(xs, ys, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76f8ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show() #此句可有可无"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "392f5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I made a poetry machine pound party for friends and relations relations her a call call party a man give her a call call party\n"
     ]
    }
   ],
   "source": [
    "'''test: 输入一个sequence, model就会输出下一个值, \n",
    "既而生成poetry(take a sequence and get next value, add that sequence pass that model get next value and add that sequence and so on\n",
    "(输入一个序列, 得到下一个值, 将其添加到序列中, 继续传递给模型得到下一个值, 再将其添加到序列中, 以此类推))'''\n",
    "\n",
    "'''若输入一个model之前未见过的单词序列, 它可粗略推理出下一个单词是什么\n",
    "(因此为了让模型生成文本, 可输入一些单词并预测下一个值,然后将输出添加到单词字符串中进而预测下一个值, 以此类推)'''\n",
    "seed_text = 'I made a poetry machine'\n",
    "next_words = 20\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    #注:[token_list]中, token_list本身是list(列表)有'[]', 外面再加一个'[]'是要使其成为array(数组), 因为TensorFlow 2.x中array才能放入model训练\n",
    "    #maxlen = max_sequence_len-1因为测试集中的X_test(测试数据)是去掉y_test(测试label)后的前面部分\n",
    "    token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding = 'pre')\n",
    "    #model.predict_classes is deprecated from tensorflow in version 2.6\n",
    "    #predicted = model.predict_classes(token_list, verbose = 0) \n",
    "    #axis = 1（按列）或 -1(最后一列) (因为model.predict(token_list)的结果就为一列); 另, verbose大多数情况下默认为1\n",
    "    predicted = np.argmax(model.predict(token_list, verbose = 0), axis=-1)\n",
    "    output_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += ' ' + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
