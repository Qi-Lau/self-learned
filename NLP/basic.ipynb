## Tokenization(分词): 
**用计算机可处理的方式来表示单词, 并在后期训练一个神经网络, 来理解这些单词的含义**
<br>
例如, 一个单词可以用分开的每个字母表示, 每个字母用ASCII(一种主流编码方式)转换成数字(计算机可处理), 但有时会遇到不同单词用同样字母表示, 如silent和listen, 即可用同样数字表示, 只是数字顺序不同. 因此, **仅通过字母理解一个单词的含义并不容易. 所以, 跟编码字母比起来, 编码单词可能更简单**
<br>
### 下面是对句子进行分词的部分代码

import tensorflow as tf
from tensorflow import keras
#获取Tokenizer API
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat'
]

#创建一个Tokenizer Object实例, 参数num_words表示要保留的最大单词数(指保留要分词的语料库中(如此例中是sentences)出现最频繁的100个单词)
tokenizer = Tokenizer(num_words = 100)
# 训练NN, 使Tokenizer(分词器)可查看所有文本并将文本与对应数字进行匹配
tokenizer.fit_on_texts(sentences) 
# 完整的单词列表可通过分词器word_index属性获得
word_index = tokenizer.word_index 
print(word_index)

#Tokenizer很聪明, 可识别例外, 如更新句子增加第三个句子
sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!'
]

#创建一个Tokenizer Object实例, 参数num_words表示要保留的最大单词数(指保留要分词的语料库中(如此例中是sentences)出现最频繁的100个单词)
tokenizer = Tokenizer(num_words = 100)
# 训练NN, 使Tokenizer(分词器)可查看所有文本并将文本与对应数字进行匹配
tokenizer.fit_on_texts(sentences) 
# 完整的单词列表可通过分词器word_index属性获得
word_index = tokenizer.word_index 
print(word_index)

## Sequencing-Turning sentences into data(正则化)
**前面内容了解了如何导入一组句子, 并用Tokenizer(分词器)将单词转化为数字或称为token
<br>下面为句子创建sequences of numbers(数字序列), 然后用工具对其进行处理, 从而为NN做准备**

sentences2 = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences2) 
word_index = tokenizer.word_index 

# 创建了代表每个句子的token sequence(序列)
sequences = tokenizer.texts_to_sequences(sentences2) 

print(word_index)
print(sequences)

# 测试Tokenizer(分词器)对未出现在原始数据集(training set)中单词的反应, 发现Tokenizer confused.
test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]

test_seq = tokenizer.texts_to_sequences(test_data)

print(test_seq)

# 防止上述损失序列长度的事件发生, 在Tokenizer function中加入参数 oov_token = '<OOV>', means 设置为代替语料库中无法识别的内容(或未见过的单词)
tokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>') #注: <OOV> 是 1
tokenizer.fit_on_texts(sentences2) 
word_index = tokenizer.word_index 

# 创建了代表每个句子的token sequence(序列)
sequences = tokenizer.texts_to_sequences(sentences2) 
test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]

test_seq = tokenizer.texts_to_sequences(test_data)

print(test_seq)
print(word_index)

'''when train NN, how to address sentences with different lengths?
   对于图像而言, 通常尺寸相同. 而sentences, 进阶的解决方案:借助不规则张量(Ragged Tensor), 另一解决方案:padding, 见下方函数 pad_sequences'''
from tensorflow.keras.preprocessing.sequence import pad_sequences

sentences2 = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>')
tokenizer.fit_on_texts(sentences2) 
word_index = tokenizer.word_index 

# 创建了代表每个句子的token sequence(序列)
sequences = tokenizer.texts_to_sequences(sentences2) 

# (用0)填充sequence(序列), 以确保所有句子序列长度都一致, 默认在序列前面补0 且 默认填充序列的长度与最大长度一致
padded = pad_sequences(sequences) 
#padded = pad_sequences(sequences, padding = 'post') # 在序列后填充0
# 假设不希望填充序列的长度与最大长度一致, 用maxlen参数指定所需的序列长度. 若有序列长度超过maxlen设置如何, 实际上, 你可设置截断方式truncating
#padded = pad_sequences(sequences, padding = 'post', truncating = 'post', maxlen = 5) # post truncation截断结尾处的单词; pre truncation截断开头的单词

print(word_index)
print(sequences)
print(padded)

## Training a model to recognize sentiment in text
**前面都是对文本数据的预处理,<br>下面学习如何构建一个能识别文本情感的分类器**

import json #导入JSON库

#用JSON库加载Sarcasm的json文件. 注:在训练前, 要先把数据转换为Python格式, 即每个JSON元素都会变成一个Python列表元素(用list的'[]'封装起来)
with open('Sarcasm.json', 'r') as f:
    datastore = json.loads('[' + 
        f.read().replace('}\n{', '},\n{') + 
    ']')

#分别创建三个list(列表)
sentences3 = []
labels = []
urls = []
#当对json文件迭代时, 可将所需值加载到Python列表中
for item in datastore:
    sentences3.append(item['headline'])
    labels.append(item['is_sarcastic'])
    urls.append(item['article_link'])

#对文本进行预处理
tokenizer = Tokenizer(oov_token = '<OOV>')
tokenizer.fit_on_texts(sentences3) 
word_index = tokenizer.word_index 

sequences2 = tokenizer.texts_to_sequences(sentences3) 

padded1 = pad_sequences(sequences2, padding = 'post')

print(padded1[0])
print(padded1.shape) #(26709, 40)一共26709个序列, 每个序列有40个token

vocab_size = 10000
embedding_dim = 16
max_length = 100
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
training_size = 20000

#split training data and test data
training_sentences = sentences3[0:training_size]
testing_sentences = sentences3[training_size:]
training_labels = labels[0:training_size]
testing_labels = labels[training_size:]

#之前用Tokenizer(分词器)创建一个包含所有单词的单词索引, 但真要测试其有效性, 须保证NN只见过training data而没见过test data. 所以要确保Tokenizer仅用于training data
tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)
tokenizer.fit_on_texts(training_sentences) 
word_index = tokenizer.word_index 

training_sequences = tokenizer.texts_to_sequences(training_sentences) 

training_padded = pad_sequences(training_sequences, maxlen = max_length,
                                padding = padding_type, truncating = trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences) 

testing_padded = pad_sequences(testing_sequences, maxlen = max_length,
                                padding = padding_type, truncating = trunc_type)

# Need this block to get it to work with TensorFlow 2.x
# 在TensorFlow 2.x中, 要输入训练的数据格式须先用函数np.aray()从list转化为array, 才能放入model中训练
import numpy as np
training_padded = np.array(training_padded)
training_labels = np.array(training_labels)
testing_padded = np.array(testing_padded)
testing_labels = np.array(testing_labels)

model = tf.keras.Sequential([
    #如何从token中了解句子意义, 仅靠数字，如何判断某个headline是否有讽刺性?
    #将一组单词输入到完成训练的NN中, NN可查看这些词的vector(向量), 并将向量相加, 输出一个情感判断,这个概念为embedding(嵌入)
    #Embedding:where the direction of each word(每个单词的情感方向) will be learned epoch by epoch
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),
    #进行pooling(池化), 即把前面的向量相加
    tf.keras.layers.GlobalAveragePooling1D(),
    #然后将其feed into普通的深度神经网络中 
    tf.keras.layers.Dense(24, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])

model.compile(
    loss = 'binary_crossentropy',
    optimizer = 'adam',
    metrics = ['accuracy']

)

num_epochs = 30

history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)

#利用上述训练好的model(或NN), 判断新句子的情感色彩
newsentence = [
    'granny starting to fear spiders in the garden might be real',
    'the weather today is bright and sunny'
]
#预测结果表明第一个句子判定为sarcastic的概率极高(接近1), 第二个句子判定为sarcastic的概率极低

#借助之前创建的Tokenizer(分词器), 将句子转换为sequence(序列), 这样单词将拥有和训练集一样的token
newsequences = tokenizer.texts_to_sequences(newsentence)

#然后将这些sequence(序列)pad(填充)到跟训练集相同的维度, 并使用同样的padding_type
newpadded = pad_sequences(newsequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)

#在填充后的数据集上进行预测
print(model.predict(newpadded))
