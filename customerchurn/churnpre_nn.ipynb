# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import numpy as np
import matplotlib
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
#Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.
from mlxtend.plotting import plot_confusion_matrix 

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('D:/R/Pythonfile'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        
# Any results you write to the current directory are saved as output.

'''Only importing Keras stuff: They will start with ‘Keras.’ e.g. from keras.models import…
Only importing TensorFlow Keras libraries, these start with TensorFlow.Keras. e.g. from TensorFlow.Keras.models import…'''

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout

from keras.constraints import maxnorm

# Data exploration
bank_df = pd.read_csv('D:/R/Pythonfile\Churn_Modelling.csv')
bank_df.head()

X = bank_df.drop(labels=['customerId', 'surname', 'rowNumber', 'churn'], axis = 1)
y = bank_df['churn']
X.head()

X.isna().any() #The any() function returns True if any item in an iterable are true, otherwise it returns False. If the iterable object is empty, the any() function will return False.

X.isna()

y.head()

# Handling categorical values
label1 = LabelEncoder()
X['geography'] = label1.fit_transform(X['geography'])
label = LabelEncoder()
X['gender'] = label.fit_transform(X['gender'])
X.head()

X = pd.get_dummies(X, drop_first = True, columns = ['geography']) 
'''Convert categorical variable into dummy/indicator variables. 
drop_first: Whether to get k-1 dummies out of k categorical levels by removing the first level.'''
X.head()

#Feature Scaling and test train split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#ANN implementation
#第一种格式
#Mostly you’ll be using sequential models. And most of the time you will be using either Dense or LSTM layers
model = Sequential()
#The first parameter:units are the number of nodes (or circles/first layer of the hidden layers)
#input_dim argument it must match the amount of columns in our X_train dataset, we can get it by X.shape[1] easily
#input_dim = X.shape[1] 可和 input_shape=[X.shape[1]] 互换
#kernel_constraint:This deals with the scaling of the weights(Constraint function applied to the kernel weights matrix)
#maxnorm:MaxNorm weight constraint:Constrains the weights incident to each hidden unit to have a norm less than or equal to a desired value
model.add(Dense(units = X.shape[1], activation = 'relu', input_dim = X.shape[1], kernel_constraint = maxnorm(3)))
model.add(Dropout(rate = 0.3))
model.add(Dense(128, activation = 'relu', kernel_constraint = maxnorm(3)))
model.add(Dropout(0.3))
model.add(Dense(1, activation = 'sigmoid'))
'''For a single (yes/no) classification model we use ‘sigmoid’ there are many different functions available, 
for example if we are building a network that classifies multiple outcomes (for example grouping customers into specific groups) we may use a ‘softmax’ activation function.'''


#第二种格式
# from tensorflow.keras import layers

# model = keras.Sequential([
#     # the hidden ReLU layers
#     layers.Dense(units=X.shape[1], activation='relu', input_shape=[X.shape[1]], kernel_constraint = maxnorm(3)),
#     layers.Dropout(rate = 0.3),
#     layers.Dense(units=128, activation='relu', kernel_constraint = maxnorm(3)),
#     # the linear output layer 
#     layers.Dropout(0.3),
#     layers.Dense(units=1, activation = 'sigmoid'), #最后一个逗号可有可无
# ])

X.shape[1]

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

history = model.fit(X_train, y_train.to_numpy(), validation_data=(X_test, y_test), batch_size = 10, epochs = 10, verbose = 1)
#more epochs can mean more accuracy, but also more processing time, also too many epochs can lead to overfitting. 
# A smaller batch size is less accurate but quicker.

print(tf.__version__)

print(tf.version.VERSION)

#查看二分类预测的类别(整数int). 因为model.predict(X_test)结果为概率(小数float)
#model.predict_classes is deprecated from tensorflow in version 2.6
#第一种方法
#y_pred = model.predict(X_test)
#y_predclass= np.where(y_pred > 0.5, 1,0)
#y_predclass
#第二种方法
(model.predict(X_test) > 0.5).astype('int64') # 'int32'或'int64'都可

#补充知识(查看多类分类或多标签分类的预测的类别):
'''
Mutli-class Classification:
Select the class with the highest probability

np.argmax(model.predict(x_test), axis=-1) #axis = 1（按列）或 -1(最后一列) (因为model.predict(x_test)的结果就为一列)

Multi-label Classification:
Where you can have multiple output classes per example, use a threshold to select which labels apply.

y_pred = model.predict(x_test, axis=1)
[i for i,prob in enumerate(y_pred) if prob > 0.5]
'''

y_test

#Returns the loss value & metrics values for the model in test mode. 计算是分批(in batches)完成的
model.evaluate(X_test, y_test.to_numpy())

#'int32'或'int64'都可
#y_predclass = (model.predict(X_test) > 0.5).astype('int32') 
y_predclass = (model.predict(X_test) > 0.5).astype('int64')
confusion_matrix(y_test, y_predclass) #因为Classification metrics can't handle a mix of binary and continuous targets

accuracy_score(y_test, y_predclass) #因为Classification metrics can't handle a mix of binary and continuous targets

#Visualising the results
plt.plot(history.history['accuracy']) 
plt.plot(history.history['val_accuracy']) 
plt.title('model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper left') 
plt.show()

'''to save the model, this is so that we can deploy it to production 
later — AWS SageMaker / Azure / Google all have different ways to do this, 
but generally youll need both a JSON and a WEIGHTS file'''

# serialize model to JSON 
model_json = model.to_json() 
with open("model.json", "w") as json_file:
    json_file.write(model_json) 
    
# serialize weights to HDF5 
model.save_weights("model.h5") 
print("Saved model to disk")
